import os
import mlflow
import mlflow.sklearn
import pandas as pd
import joblib
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report

# D√©finition des fichiers pour sauvegarder le mod√®le et le scaler
MODEL_FILENAME = "models/model.pkl"
SCALER_FILENAME = "models/scaler.pkl"
FEATURES_FILENAME = "models/model_features.pkl"

# S√©lection des 16 features √† utiliser
SELECTED_FEATURES = [
    "Account length", "Area code", "Customer service calls", "International plan",
    "Number vmail messages", "Total day calls", "Total day charge", "Total day minutes",
    "Total night calls", "Total night charge", "Total night minutes",
    "Total eve calls", "Total eve charge", "Total eve minutes",
    "Total intl calls", "Voice mail plan"
]

def prepare_data(data_path='merged_churn.csv'):
    """Chargement, pr√©traitement et s√©paration des donn√©es."""
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"‚ùå Erreur : le fichier {data_path} est introuvable.")

    data = pd.read_csv(data_path)
    
    # Encodage des variables cat√©goriques
    label_encoder = LabelEncoder()
    for col in ["International plan", "Voice mail plan", "Churn"]:
        data[col] = label_encoder.fit_transform(data[col])
    
    X = data[SELECTED_FEATURES]
    y = data['Churn']
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)
    
    # Cr√©ation du dossier 'models' s'il n'existe pas
    os.makedirs("models", exist_ok=True)
    joblib.dump(scaler, SCALER_FILENAME)
    joblib.dump(SELECTED_FEATURES, FEATURES_FILENAME)
    
    print(f"‚úÖ Donn√©es pr√©par√©es : {X_train.shape[0]} √©chantillons d'entra√Ænement, {X_test.shape[0]} de test.")
    return X_train, X_test, y_train, y_test

def train_model(X_train, y_train):
    """Entra√Æne un mod√®le Gradient Boosting et l'enregistre avec MLflow."""
    
    # üö® Ferme tout run MLflow actif pour √©viter l'erreur
    mlflow.end_run()

    with mlflow.start_run():
        gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, subsample=1.0, random_state=42)
        gb_model.fit(X_train, y_train)

        # Enregistrement des hyperparam√®tres
        mlflow.log_param("n_estimators", 100)
        mlflow.log_param("learning_rate", 0.1)
        mlflow.log_param("subsample", 1.0)

        # Enregistrement du mod√®le
        save_model(gb_model)
        mlflow.sklearn.log_model(gb_model, "model")
        mlflow.log_artifact(MODEL_FILENAME)

        print("‚úÖ Mod√®le entra√Æn√© et enregistr√© avec MLflow.")
        return gb_model

def evaluate_model(model, X_test, y_test):
    """√âvalue le mod√®le et enregistre la m√©trique d'accuracy dans MLflow."""
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)

    print("\nüìä Classification Report :")
    print(classification_report(y_test, y_pred))
    print(f"‚úÖ Pr√©cision du mod√®le : {accuracy:.4f}")
    mlflow.end_run()  # Ferme tout run actif avant d'en d√©marrer un nouveau

    with mlflow.start_run():
        mlflow.log_metric("accuracy", accuracy)
        mlflow.log_artifact(FEATURES_FILENAME)

    return accuracy

def save_model(model, filename=MODEL_FILENAME):
    """Enregistre le mod√®le dans un fichier pickle."""
    joblib.dump(model, filename)
    print(f"‚úÖ Mod√®le enregistr√© sous {filename}")

def load_model(filename=MODEL_FILENAME):
    """Charge un mod√®le enregistr√©."""
    if not os.path.exists(filename):
        raise FileNotFoundError(f"‚ùå Le fichier {filename} est introuvable.")
    print(f"‚úÖ Mod√®le charg√© depuis {filename}")
    return joblib.load(filename)

